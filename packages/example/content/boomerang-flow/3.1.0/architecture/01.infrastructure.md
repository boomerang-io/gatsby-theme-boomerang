---
title: Infrastructure Architecture
index: 1
---

# Infrastructure architecture

The infrastructure is based on top of Kubernetes<sup>Â®</sup> using Jobs, ConfigMaps, Secrets, and Persistent Volumes.

## Kubernetes

### Nodes

The workflow tasks will run as Jobs on any node unless dedicated nodes are implemented using:

- Node taint: `dedicated=bmrg-worker:NoSchedule`
- Node label: `node-role.kubernetes.io/bmrg-worker=true`

### Ephemeral storage

As with all containers, there is ephemeral storage used that we have limited to 8GB by default. This impacts the number of workers that can be running in parallel, based on the amount of primary disk used. This is important. 

Flow workers have a setting to delete completed workers, if this is not enabled, then the completed workers stick around and use up the available ephemeral storage.

See [Kubernetes ephemeral storage](https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#local-ephemeral-storage) reference information.

### Persistent volumes

There are different types of persistent volumes used by the worker orchestration system.

You can configure the storage size, storage class, and access modes for the following types in the Settings under Administer. By default:
- Storage Size is set to 1GB
- Storage Class is not set. This will therefore use the default defined in your kubernetes cluster
- Access Mode is set as Read Write Many.

**Workspaces**

The workspaces persistent volume is spun up per Boomerang CICD Component and is used as a persistent cache between executions. Whilst these do not churn nearly as often, they do add up i.e. if you have 100 Boomerang CICD Components you will have a requirement of 100GB (100 x xGB) of Persistent Storage available.

This is mapped to the `/workspace` mounted storage in the Tasks in a Workflow.

> **Note:** Workspaces is not yet able to be enabled via the Workflows UI.

**Workflows**

The workflows persistent volume is per workflow execution and is used as a per execution storage. This can cause quite a churn of persistent volumes and can cause instability depending on the Storage driver you are using.

This is mapped to the `/workflow` mounted storage in the Tasks in a Workflow.

We recommend using [Ranchers Local Path Provisioner](https://github.com/rancher/local-path-provisioner) on the nodes executing Tasks as this allows for dynamic provisioning of local disk, that if SSD, allows for low latency high speed writes.

### Pod Anti-affinity

If dedicated nodes are enabled, a pod soft anti affinity is also enabled to ensure that workers are attempted to be balanced across nodes as best as possible.
